{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzDLhhq3nTFWXKNP+zg9ee",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmehtiyev2019/llm-api-tm/blob/main/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--ZLnFjA69gV",
        "outputId": "28e3d3d0-dacf-4bdd-97fc-8fba5ca7fb91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.7-py3-none-any.whl (221 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.7\n"
          ]
        }
      ],
      "source": [
        "# Import the OpenAI library to interact with the LLM\n",
        "!pip install openai\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up my OpenAI API key.\n",
        "openai.api_key = 'test-api-key'\n",
        "\n",
        "# Initial Setup\n",
        "# Here, I'm defining a basic function schema as a starting point.\n",
        "# This is just a template and I'll modify it according to my project needs later.\n",
        "function_schema = {\n",
        "    \"name\": \"sampleFunction\",\n",
        "    \"description\": \"Template for function schema.\",\n",
        "    \"parameters\": {\n",
        "        \"param1\": \"string\",\n",
        "        \"param2\": \"number\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Function to Process Natural Language Input\n",
        "# This function takes user input and uses the LLM to process it.\n",
        "def process_user_input(input_text):\n",
        "    \"\"\"\n",
        "    Takes user input as text and gets a response from the LLM.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Generating a response from the LLM. I'll choose the model based on my project's requirement.\n",
        "        response = openai.Completion.create(\n",
        "            model=\"text-davinci-003\",  # This is just an example model.\n",
        "            prompt=input_text,\n",
        "            max_tokens=150\n",
        "        )\n",
        "        return response.choices[0].text.strip()\n",
        "    except Exception as e:\n",
        "        # Just in case something goes wrong, this will tell me what it is.\n",
        "        print(f\"Encountered an error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Testing the Function\n",
        "# I'll use this part to test how my function is working with a sample input.\n",
        "user_input = \"How do I write a Python function to calculate factorial?\"\n",
        "\n",
        "# Getting the LLM's response to my input\n",
        "llm_response = process_user_input(user_input)\n",
        "print(\"Response from LLM:\", llm_response)\n",
        "\n",
        "# Next Steps: Translating LLM Response to API Call\n",
        "# This is where I'll develop the logic to convert the LLM's response into an actual API call.\n",
        "# The specifics will depend on what API I decide to work with.\n",
        "\n",
        "# Placeholder for future code related to API call translation.\n",
        "# ...\n",
        "\n",
        "# Wrapping up the initial setup. More to come as I develop this project further!\n",
        "print(\"Basic setup is ready!\")\n"
      ],
      "metadata": {
        "id": "CAWzU5Oc8-Yz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}